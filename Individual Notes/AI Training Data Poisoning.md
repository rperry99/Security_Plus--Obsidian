---
Area: "[[Cyber Security]]"
tags:
  - Note
aliases:
---
# What is it?
- This is when an attacker will try to mess up an AI by feeding the it bad training data to make the AI act incorrectly. 

# Real World Example
- Microsoft launched an AI chatter bot named Tay (Thinking About You).
- It joined Twitter on March 23, 2016.
- It was designed to learn by interacting with Twitter Users.
- Microsoft didn't program any anti-offensive behavior, so Tay quickly became racist, sexist, and inappropriate.


---
# Related Notes
```dataview
list
from [[]] and #Note 
sort file.name asc
```
